{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Extracting', '/mnt/hdd1/thkim/dataset/MNIST/train-images-idx3-ubyte.gz')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/gzip.py:268: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  chunk = self.extrabuf[offset: offset + size]\n",
      "input_data.py:42: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  data = data.reshape(num_images, rows, cols, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Extracting', '/mnt/hdd1/thkim/dataset/MNIST/train-labels-idx1-ubyte.gz')\n",
      "('Extracting', '/mnt/hdd1/thkim/dataset/MNIST/t10k-images-idx3-ubyte.gz')\n",
      "('Extracting', '/mnt/hdd1/thkim/dataset/MNIST/t10k-labels-idx1-ubyte.gz')\n",
      "('-1', 9.3404226)\n",
      "Iter 100 5.7818 42.3646 2.6671\n",
      "Iter 200 5.31755 41.56 2.20288\n",
      "Iter 300 5.14726 44.2507 2.03263\n",
      "Iter 400 4.96252 45.7553 1.84792\n",
      "===================================Test iter 0 5.078 42.902 1.963 0.500000\n",
      "Iter 100 4.9746 42.2676 1.86004\n",
      "Iter 200 4.78539 43.0351 1.67086\n",
      "Iter 300 4.66904 44.7163 1.55454\n",
      "Iter 400 4.59508 46.552 1.48061\n",
      "===================================Test iter 1 4.678 44.158 1.564 0.500000\n",
      "Iter 100 4.66112 43.3532 1.54669\n",
      "Iter 200 4.49448 45.6154 1.38007\n",
      "Iter 300 4.37479 47.8315 1.26041\n",
      "Iter 400 4.35589 49.7237 1.24153\n",
      "===================================Test iter 2 4.425 47.443 1.311 0.500000\n",
      "Iter 100 4.45662 46.4063 1.3423\n",
      "Iter 200 4.30212 49.4044 1.18781\n",
      "Iter 300 4.1772 51.9722 1.06292\n",
      "Iter 400 4.18921 53.6947 1.07495\n",
      "===================================Test iter 3 4.254 51.409 1.140 0.500000\n",
      "Iter 100 4.3165 50.0327 1.20227\n",
      "Iter 200 4.16772 53.4957 1.05351\n",
      "Iter 300 4.03408 56.4736 0.91989\n",
      "Iter 400 4.06729 57.8906 0.953126\n",
      "===================================Test iter 4 4.131 55.463 1.017 0.500000\n",
      "Iter 100 4.21322 53.8051 1.09908\n",
      "Iter 200 4.07012 57.5693 0.956003\n",
      "Iter 300 3.92574 60.8513 0.811645\n",
      "Iter 400 3.97461 62.051 0.860534\n",
      "===================================Test iter 5 4.039 59.435 0.925 0.500000\n",
      "Iter 100 4.13482 57.4299 1.02077\n",
      "Iter 200 3.9966 61.4579 0.882566\n",
      "Iter 300 3.84347 65.0119 0.729446\n",
      "Iter 400 3.90183 66.05 0.787825\n",
      "===================================Test iter 6 3.967 63.170 0.853 0.500000\n",
      "Iter 100 4.07337 60.8384 0.959386\n",
      "Iter 200 3.93964 65.1203 0.825682\n",
      "Iter 300 3.77975 68.8949 0.665803\n",
      "Iter 400 3.84497 69.796 0.731046\n",
      "===================================Test iter 7 3.909 66.598 0.795 0.500000\n",
      "Iter 100 4.02394 64.0105 0.910033\n",
      "Iter 200 3.89358 68.5371 0.779692\n",
      "Iter 300 3.72865 72.5337 0.614776\n",
      "Iter 400 3.79874 73.3071 0.684886\n",
      "===================================Test iter 8 3.863 69.768 0.749 0.500000\n",
      "Iter 100 3.98384 66.957 0.870007\n",
      "Iter 200 3.85583 71.6962 0.742011\n",
      "Iter 300 3.6871 75.9209 0.57329\n",
      "Iter 400 3.76108 76.5878 0.647288\n",
      "===================================Test iter 9 3.825 72.707 0.711 0.500000\n",
      "Iter 100 3.95088 69.6959 0.837105\n",
      "Iter 200 3.82444 74.6058 0.710681\n",
      "Iter 300 3.65269 79.0626 0.538936\n",
      "Iter 400 3.72995 79.6086 0.616219\n",
      "===================================Test iter 10 3.793 75.406 0.679 0.500000\n",
      "Iter 100 3.92307 72.2235 0.809349\n",
      "Iter 200 3.79787 77.3181 0.684167\n",
      "Iter 300 3.62415 81.9755 0.51046\n",
      "Iter 400 3.70387 82.4304 0.590186\n",
      "===================================Test iter 11 3.765 77.884 0.651 0.500000\n",
      "Iter 100 3.89891 74.5691 0.785247\n",
      "Iter 200 3.77515 79.8399 0.661493\n",
      "Iter 300 3.60028 84.684 0.486645\n",
      "Iter 400 3.68164 85.0581 0.568009\n",
      "===================================Test iter 12 3.742 80.184 0.628 0.500000\n",
      "Iter 100 3.87794 76.7473 0.764325\n",
      "Iter 200 3.75528 82.1922 0.641684\n",
      "Iter 300 3.57969 87.2007 0.466102\n",
      "Iter 400 3.66247 87.4967 0.548889\n",
      "===================================Test iter 13 3.721 82.308 0.607 0.500000\n",
      "Iter 100 3.85957 78.7596 0.746004\n",
      "Iter 200 3.73778 84.376 0.624227\n",
      "Iter 300 3.56172 89.5353 0.448178\n",
      "Iter 400 3.64585 89.7637 0.532317\n",
      "===================================Test iter 14 3.703 84.283 0.590 0.500000\n",
      "Iter 100 3.84337 80.6191 0.729856\n",
      "Iter 200 3.72234 86.4093 0.608832\n",
      "Iter 300 3.54599 91.7183 0.432494\n",
      "Iter 400 3.63139 91.8825 0.517903\n",
      "===================================Test iter 15 3.687 86.123 0.574 0.500000\n",
      "Iter 100 3.82897 82.3422 0.715496\n",
      "Iter 200 3.70854 88.3054 0.595074\n",
      "Iter 300 3.53215 93.7451 0.418697\n",
      "Iter 400 3.61857 93.8548 0.505124\n",
      "===================================Test iter 16 3.673 87.829 0.560 0.500000\n",
      "Iter 100 3.8161 83.9452 0.702663\n",
      "Iter 200 3.69627 90.0639 0.582839\n",
      "Iter 300 3.51996 95.6296 0.406544\n",
      "Iter 400 3.6071 95.6908 0.493691\n",
      "===================================Test iter 17 3.661 89.417 0.547 0.500000\n",
      "Iter 100 3.80452 85.4369 0.691125\n",
      "Iter 200 3.6853 91.6983 0.571909\n",
      "Iter 300 3.50911 97.3888 0.395727\n",
      "Iter 400 3.59675 97.4063 0.483376\n",
      "===================================Test iter 18 3.649 90.897 0.536 0.500000\n",
      "Iter 100 3.79398 86.8297 0.680616\n",
      "Iter 200 3.67539 93.2256 0.562038\n",
      "Iter 300 3.49931 99.0363 0.385966\n",
      "Iter 400 3.58731 99.0103 0.473972\n",
      "===================================Test iter 19 3.639 92.281 0.526 0.500000\n",
      "Iter 100 5.75023 38.6731 0.740288\n",
      "Iter 200 5.4992 34.6075 0.69004\n",
      "Iter 300 5.34794 35.9049 0.462562\n",
      "Iter 400 5.28336 32.4804 0.574839\n",
      "===================================Test iter 20 5.080 28.338 0.579 0.450000\n",
      "Iter 100 6.11797 22.7034 0.816021\n",
      "Iter 200 6.03196 22.1963 0.776818\n",
      "Iter 300 5.99178 23.5517 0.581602\n",
      "Iter 400 5.76583 20.2538 0.697004\n",
      "===================================Test iter 21 5.513 17.745 0.695 0.400000\n",
      "Iter 100 6.19293 15.3237 0.919123\n",
      "Iter 200 6.12722 14.7918 0.935698\n",
      "Iter 300 6.15496 15.961 0.762038\n",
      "Iter 400 5.87146 13.4509 0.87148\n",
      "===================================Test iter 22 5.689 12.274 0.864 0.350000\n",
      "Iter 100 6.18854 11.1531 1.0561\n",
      "Iter 200 6.16669 10.8324 1.10897\n",
      "Iter 300 6.23342 11.7905 0.952894\n",
      "Iter 400 5.95819 9.99334 1.05815\n",
      "===================================Test iter 23 5.833 9.428 1.042 0.300000\n",
      "Iter 100 6.20358 8.75615 1.20208\n",
      "Iter 200 6.18313 8.485 1.26522\n",
      "Iter 300 6.27985 9.25562 1.13732\n",
      "Iter 400 6.03866 8.0104 1.23084\n",
      "===================================Test iter 24 5.959 7.754 1.210 0.250000\n",
      "Iter 100 6.18903 7.12972 1.33886\n",
      "Iter 200 6.18977 6.97297 1.40711\n",
      "Iter 300 6.28442 7.53796 1.30021\n",
      "Iter 400 6.07378 6.64815 1.38065\n",
      "===================================Test iter 25 6.043 6.617 1.351 0.200000\n",
      "Iter 100 6.17645 6.058 1.45122\n",
      "Iter 200 6.15967 5.87291 1.52509\n",
      "Iter 300 6.24611 6.27863 1.43963\n",
      "Iter 400 6.0657 5.63557 1.50835\n",
      "===================================Test iter 26 6.088 5.770 1.469 0.150000\n",
      "Iter 100 6.13563 5.237 1.54671\n",
      "Iter 200 6.08536 4.98143 1.63332\n",
      "Iter 300 6.17633 5.31057 1.56553\n",
      "Iter 400 6.03928 4.87634 1.62662\n",
      "===================================Test iter 27 6.108 5.119 1.579 0.100000\n",
      "Iter 100 6.07333 4.57839 1.6369\n",
      "Iter 200 5.92968 4.12624 1.74569\n",
      "Iter 300 6.07842 4.51817 1.69548\n",
      "Iter 400 5.92185 4.09402 1.75786\n",
      "===================================Test iter 28 6.057 4.467 1.699 0.050000\n",
      "Iter 100 5.90745 3.85666 1.73282\n",
      "Iter 200 5.75326 3.41438 1.86675\n",
      "Iter 300 5.9209 3.79772 1.81873\n",
      "Iter 400 5.80671 3.50632 1.88178\n",
      "===================================Test iter 29 5.898 3.752 1.819 0.000000\n",
      "Iter 100 5.62105 3.19765 1.81913\n",
      "Iter 200 5.5451 2.90177 1.96314\n",
      "Iter 300 5.6444 3.14884 1.9147\n",
      "Iter 400 5.57845 2.95873 1.97292\n",
      "===================================Test iter 30 5.628 3.120 1.911 0.000000\n",
      "Iter 100 5.40397 2.69448 1.88824\n",
      "Iter 200 5.37227 2.48314 2.0362\n",
      "Iter 300 5.42946 2.64442 1.98932\n",
      "Iter 400 5.39937 2.53302 2.04056\n",
      "===================================Test iter 31 5.422 2.640 1.979 0.000000\n",
      "Iter 100 5.24106 2.31546 1.94153\n",
      "Iter 200 5.23536 2.1549 2.09071\n",
      "Iter 300 5.27507 2.28021 2.04483\n",
      "Iter 400 5.2592 2.20266 2.09066\n",
      "===================================Test iter 32 5.276 2.296 2.031 0.000000\n",
      "Iter 100 5.12272 2.03682 1.98354\n",
      "Iter 200 5.13724 1.91825 2.13117\n",
      "Iter 300 5.16475 2.01899 2.08546\n",
      "Iter 400 5.15744 1.96245 2.12739\n",
      "===================================Test iter 33 5.166 2.038 2.069 0.000000\n",
      "Iter 100 5.03778 1.8343 2.01626\n",
      "Iter 200 5.06647 1.74672 2.16122\n",
      "Iter 300 5.08561 1.8288 2.11745\n",
      "Iter 400 5.08145 1.78367 2.15426\n",
      "===================================Test iter 34 5.086 1.850 2.096 0.000000\n",
      "Iter 100 4.97458 1.68198 2.04222\n",
      "Iter 200 5.01366 1.61735 2.18502\n",
      "Iter 300 5.02618 1.68524 2.1422\n",
      "Iter 400 5.02379 1.64753 2.17513\n",
      "===================================Test iter 35 5.025 1.708 2.118 0.000000\n",
      "Iter 100 4.92568 1.5631 2.06335\n",
      "Iter 200 4.97184 1.51439 2.2044\n",
      "Iter 300 4.98017 1.57355 2.1619\n",
      "Iter 400 4.9784 1.54063 2.1913\n",
      "===================================Test iter 36 4.977 1.595 2.135 0.000000\n",
      "Iter 100 4.88691 1.46857 2.08039\n",
      "Iter 200 4.93683 1.4289 2.21991\n",
      "Iter 300 4.94309 1.48392 2.17741\n",
      "Iter 400 4.94108 1.4531 2.20424\n",
      "===================================Test iter 37 4.938 1.504 2.148 0.000000\n",
      "Iter 100 4.8555 1.39105 2.09513\n",
      "Iter 200 4.90713 1.35656 2.23289\n",
      "Iter 300 4.91227 1.40982 2.18991\n",
      "Iter 400 4.91024 1.38067 2.21503\n",
      "===================================Test iter 38 4.907 1.429 2.160 0.000000\n",
      "Iter 100 4.82938 1.32627 2.10771\n",
      "Iter 200 4.88224 1.29573 2.24398\n",
      "Iter 300 4.8862 1.34723 2.20041\n",
      "Iter 400 4.88459 1.32011 2.22431\n",
      "===================================Test iter 39 4.880 1.365 2.169 0.000000\n",
      "Iter 100 4.80742 1.27153 2.11855\n",
      "Iter 200 4.86104 1.24392 2.25341\n",
      "Iter 300 4.8639 1.2939 2.20918\n",
      "Iter 400 4.86264 1.26815 2.23241\n",
      "===================================Test iter 40 4.857 1.311 2.177 0.000000\n",
      "Iter 100 4.78862 1.22397 2.12856\n",
      "Iter 200 4.84246 1.19883 2.26139\n",
      "Iter 300 4.84458 1.2473 2.21715\n",
      "Iter 400 4.84388 1.22351 2.23954\n",
      "===================================Test iter 41 4.837 1.264 2.184 0.000000\n",
      "Iter 100 4.7724 1.18253 2.13758\n",
      "Iter 200 4.82623 1.15949 2.26829\n",
      "Iter 300 4.82734 1.20589 2.22411\n",
      "Iter 400 4.82742 1.18438 2.24579\n",
      "===================================Test iter 42 4.819 1.223 2.191 0.000000\n",
      "Iter 100 4.75826 1.14618 2.14566\n",
      "Iter 200 4.81199 1.12495 2.27438\n",
      "Iter 300 4.81181 1.16861 2.23036\n",
      "Iter 400 4.81269 1.14968 2.25106\n",
      "===================================Test iter 43 4.804 1.187 2.196 0.000000\n",
      "Iter 100 4.74576 1.11376 2.15312\n",
      "Iter 200 4.79944 1.09423 2.28001\n",
      "Iter 300 4.79805 1.13565 2.23583\n",
      "Iter 400 4.79951 1.11875 2.25564\n",
      "===================================Test iter 44 4.790 1.154 2.201 0.000000\n",
      "Iter 100 4.73469 1.08498 2.15978\n",
      "Iter 200 4.78826 1.06698 2.28493\n",
      "Iter 300 4.7857 1.10615 2.24064\n",
      "Iter 400 4.78776 1.09122 2.25971\n",
      "===================================Test iter 45 4.777 1.126 2.205 0.000000\n",
      "Iter 100 4.72466 1.05908 2.16565\n",
      "Iter 200 4.77817 1.04257 2.28919\n",
      "Iter 300 4.77463 1.0798 2.24487\n",
      "Iter 400 4.77726 1.06665 2.26328\n",
      "===================================Test iter 46 4.766 1.100 2.208 0.000000\n",
      "Iter 100 4.7155 1.0354 2.17102\n",
      "Iter 200 4.76898 1.02048 2.2929\n",
      "Iter 300 4.76443 1.05546 2.24883\n",
      "Iter 400 4.76778 1.04441 2.26659\n",
      "===================================Test iter 47 4.756 1.076 2.212 0.000000\n",
      "Iter 100 4.70711 1.01385 2.1758\n",
      "Iter 200 4.76064 1.00067 2.29606\n",
      "Iter 300 4.75521 1.03338 2.25248\n",
      "Iter 400 4.75913 1.02424 2.26947\n",
      "===================================Test iter 48 4.747 1.055 2.215 0.000000\n",
      "Iter 100 4.69949 0.994399 2.18003\n",
      "Iter 200 4.75298 0.982595 2.29883\n",
      "Iter 300 4.74685 1.01333 2.25582\n",
      "Iter 400 4.75124 1.00593 2.27201\n",
      "===================================Test iter 49 4.739 1.035 2.217 0.000000\n",
      "Iter 100 4.69254 0.976682 2.18385\n",
      "Iter 200 4.746 0.966224 2.30125\n",
      "Iter 300 4.73929 0.995137 2.25891\n",
      "Iter 400 4.744 0.989113 2.27435\n",
      "===================================Test iter 50 4.731 1.017 2.220 0.000000\n",
      "Iter 100 4.68615 0.960466 2.18731\n",
      "Iter 200 4.73957 0.951154 2.30347\n",
      "Iter 300 4.73236 0.978574 2.26164\n",
      "Iter 400 4.7373 0.973639 2.27645\n",
      "===================================Test iter 51 4.724 1.001 2.222 0.000000\n",
      "Iter 100 4.68033 0.945737 2.19041\n",
      "Iter 200 4.73365 0.937336 2.30546\n",
      "Iter 300 4.72602 0.963426 2.26411\n",
      "Iter 400 4.7311 0.959387 2.27831\n",
      "===================================Test iter 52 4.717 0.985 2.224 0.000000\n",
      "Iter 100 4.67493 0.932118 2.19325\n",
      "Iter 200 4.72823 0.924631 2.30733\n",
      "Iter 300 4.72024 0.949663 2.26632\n",
      "Iter 400 4.7254 0.946277 2.28003\n",
      "===================================Test iter 53 4.711 0.971 2.226 0.000000\n",
      "Iter 100 4.67002 0.919681 2.19587\n",
      "Iter 200 4.72321 0.912898 2.30904\n",
      "Iter 300 4.71489 0.93699 2.26831\n",
      "Iter 400 4.72013 0.934172 2.2816\n",
      "===================================Test iter 54 4.705 0.958 2.228 0.000000\n",
      "Iter 100 4.66551 0.908262 2.19828\n",
      "Iter 200 4.7185 0.901936 2.31059\n",
      "Iter 300 4.70993 0.925269 2.27011\n",
      "Iter 400 4.71525 0.92302 2.28301\n",
      "===================================Test iter 55 4.700 0.946 2.229 0.000000\n",
      "Iter 100 4.66133 0.897696 2.20048\n",
      "Iter 200 4.71413 0.891757 2.31203\n",
      "Iter 300 4.70535 0.914455 2.27178\n",
      "Iter 400 4.71074 0.912677 2.28434\n",
      "===================================Test iter 56 4.695 0.935 2.231 0.000000\n",
      "Iter 100 4.65742 0.887836 2.20255\n",
      "Iter 200 4.71006 0.882333 2.31332\n",
      "Iter 300 4.70109 0.904364 2.27335\n",
      "Iter 400 4.70654 0.903027 2.28558\n",
      "===================================Test iter 57 4.691 0.924 2.233 0.000000\n",
      "Iter 100 4.65381 0.878658 2.2045\n",
      "Iter 200 4.70626 0.873549 2.31451\n",
      "Iter 300 4.69714 0.89501 2.27482\n",
      "Iter 400 4.70263 0.894079 2.28673\n",
      "===================================Test iter 58 4.687 0.915 2.234 0.000000\n",
      "Iter 100 4.65047 0.870212 2.20628\n",
      "Iter 200 4.70276 0.865422 2.31564\n",
      "Iter 300 4.69347 0.886339 2.27616\n",
      "Iter 400 4.69899 0.885734 2.28778\n",
      "===================================Test iter 59 4.683 0.906 2.235 0.000000\n",
      "Iter 100 4.64738 0.862392 2.20792\n",
      "Iter 200 4.69951 0.857868 2.3167\n",
      "Iter 300 4.69003 0.878233 2.27738\n",
      "Iter 400 4.69558 0.877918 2.28879\n",
      "===================================Test iter 60 4.679 0.898 2.236 0.000000\n",
      "Iter 100 4.64453 0.855173 2.20944\n",
      "Iter 200 4.6965 0.850882 2.31767\n",
      "Iter 300 4.68684 0.87073 2.2785\n",
      "Iter 400 4.69236 0.87054 2.28974\n",
      "===================================Test iter 61 4.676 0.890 2.237 0.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2361ce81dccf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0minput_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mgt_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainCost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgt_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0malpha_\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Iter\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainCost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mC_acc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/mnt/hdd1/thkim/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 340\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    341\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/mnt/hdd1/thkim/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 564\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    565\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/mnt/hdd1/thkim/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 637\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    638\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/mnt/hdd1/thkim/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    642\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m       \u001b[0merror_message\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/mnt/hdd1/thkim/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    626\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m--> 628\u001b[1;33m             session, None, feed_dict, fetch_list, target_list, None)\n\u001b[0m\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import input_data\n",
    "\n",
    "WORK_DIRECTORY = '/mnt/hdd1/thkim/dataset/MNIST'\n",
    "NUM_CHANNELS = 1\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 10\n",
    "VALIDATION_SIZE = 5000  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "EVAL_BATCH_SIZE = 64\n",
    "EVAL_FREQUENCY = 100  # Number of steps between evaluations.\n",
    "mnist_width = 28\n",
    "n_visible = mnist_width * mnist_width\n",
    "n_hidden = 10\n",
    "nBatch = 128\n",
    "alpha_ = 0.5\n",
    "\n",
    "conv1_weights = tf.Variable(\n",
    "    tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n",
    "    stddev=0.1,\n",
    "    seed=SEED))\n",
    "conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "conv2_weights = tf.Variable(\n",
    "    tf.truncated_normal([5, 5, 32, 64],\n",
    "    stddev=0.1,\n",
    "    seed=SEED))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "    tf.truncated_normal(\n",
    "    [mnist_width // 4 * mnist_width // 4 * 64, 512],\n",
    "    stddev=0.1,\n",
    "    seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "fc2_weights = tf.Variable(\n",
    "    tf.truncated_normal([512, NUM_LABELS],\n",
    "    stddev=0.1,\n",
    "    seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))\n",
    "\n",
    "''' Data '''\n",
    "mnist = input_data.read_data_sets(\"/mnt/hdd1/thkim/dataset/MNIST\", one_hot=True)\n",
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
    "teX = np.reshape(teX,(len(teX),mnist_width,mnist_width,NUM_CHANNELS))\n",
    "trX = np.reshape(trX,(len(trX),mnist_width,mnist_width,NUM_CHANNELS))\n",
    "teY = np.where(teY==1)[1]\n",
    "trY = np.where(trY==1)[1]\n",
    "\n",
    "''' create node for input data '''\n",
    "#X = tf.placeholder(\"float32\", [nBatch, n_visible], name='X')\n",
    "X = tf.placeholder(\n",
    "      \"float32\",\n",
    "      shape=(nBatch, mnist_width, mnist_width, NUM_CHANNELS),name='X')\n",
    "Y = tf.placeholder(\"int64\", shape=(nBatch,),name='Y')\n",
    "\n",
    "''' create nodes for hidden variables '''\n",
    "def c_init(type='samples'):\n",
    "    if type=='samples':\n",
    "        C_init = tf.convert_to_tensor(trX[np.random.randint(0,len(trX),n_hidden)].T)\n",
    "    elif type=='random':\n",
    "        C_max = 4 * np.sqrt(6. / (n_hidden + n_hidden))\n",
    "        C_init = tf.random_uniform(shape=[n_hidden, n_hidden],\n",
    "                           minval=-C_max,maxval=C_max)\n",
    "    else:\n",
    "        C_init = None\n",
    "    return C_init\n",
    "\n",
    "C = tf.Variable(c_init('random'), name='C')\n",
    "\n",
    "conv = tf.nn.conv2d(X,\n",
    "                    conv1_weights,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='SAME')\n",
    "\n",
    "relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "pool = tf.nn.max_pool(relu,\n",
    "                      ksize=[1, 2, 2, 1],\n",
    "                      strides=[1, 2, 2, 1],\n",
    "                      padding='SAME')\n",
    "conv = tf.nn.conv2d(pool,\n",
    "                    conv2_weights,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='SAME')\n",
    "relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "pool = tf.nn.max_pool(relu,\n",
    "                      ksize=[1, 2, 2, 1],\n",
    "                      strides=[1, 2, 2, 1],\n",
    "                      padding='SAME')\n",
    "pool_shape = pool.get_shape().as_list()\n",
    "reshape = tf.reshape(\n",
    "    pool,\n",
    "    [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "\n",
    "hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases,name='hidden')\n",
    "\n",
    "FC = tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "#if train:\n",
    "#    hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "\n",
    "\n",
    "C2 = tf.expand_dims(tf.transpose(C),0)\n",
    "X2 = tf.expand_dims(FC,1)\n",
    "\n",
    "dist = tf.reduce_sum(tf.square(tf.sub(X2,C2)),2)\n",
    "loss1 = tf.reduce_mean(tf.reduce_min(dist,1))\n",
    "choice = tf.argmin(dist,1)\n",
    "\n",
    "batch = tf.Variable(0)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "      1e-5,                # Base learning rate.\n",
    "      batch * nBatch,  # Current index into the dataset.\n",
    "      trX.shape[0],          # Decay step.\n",
    "      0.95,                # Decay rate.\n",
    "      staircase=True)\n",
    "  # Use simple momentum for the optimization.\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "loss2 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(FC, Y))\n",
    "\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                  tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "\n",
    "alpha = tf.placeholder('float')\n",
    "cost = (0.5-alpha)*loss1 + (0.5+alpha)*loss2 + 5e-4 * regularizers\n",
    "\n",
    "\n",
    "train_op = tf.train.MomentumOptimizer(learning_rate,0.9).minimize(cost,global_step=batch)\n",
    "#train_op = tf.train.GradientDescentOptimizer(1e-3).minimize(cost)  # construct an optimizer\n",
    "#train_op = tf.train.AdadeltaOptimizer(1e-3).minimize(cost)\n",
    "#train_op = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "''' Launch the graph in a session '''\n",
    "sess = tf.Session()\n",
    "C_acc = np.zeros((100,10,10))\n",
    "tf.initialize_all_variables().run(session=sess)\n",
    "\n",
    "print('-1', sess.run(cost, feed_dict={X: teX[:nBatch], Y:  teY[:nBatch],alpha:alpha_}))\n",
    "C_acc[0] = sess.run(C)\n",
    "\n",
    "for i in range(1000):\n",
    "    if i== 20:\n",
    "        idx = np.random.randint(0,len(trX),128)\n",
    "        C.assign(sess.run(FC,feed_dict={X:trX[idx]})[:10])\n",
    "    if i >= 20 and alpha_ >= 0.05:\n",
    "        alpha_= alpha_ - 0.05\n",
    "    if alpha_ < 0:\n",
    "        alpha_ = 0\n",
    "    cnt = 0\n",
    "    for start, end in zip(range(0, len(trX), nBatch), range(nBatch, len(trX), nBatch)):\n",
    "        cnt += 1\n",
    "        input_ = trX[start:end]\n",
    "        gt_ = trY[start:end]\n",
    "        _,trainCost,l1,l2 = sess.run([train_op,cost,loss1,loss2], feed_dict={X: input_, Y: gt_, alpha:alpha_})\n",
    "        if cnt % 100 == 0: print \"Iter\", cnt, trainCost, l1, l2\n",
    "    if i<100: C_acc[i] = sess.run(C)\n",
    "   \n",
    "    out =  sess.run([cost,loss1,loss2,dist,choice], feed_dict={X: teX[:nBatch], Y: teY[:nBatch], alpha:alpha_})\n",
    "    print '===================================Test iter %d %5.3f %5.3f %5.3f %f'%(i, out[0], out[1], out[2], alpha_)\n",
    "    #print out[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.8112035,\n",
       " array([[ 18.52417564,   8.85062027,  29.7660408 , ...,  20.50020981,\n",
       "          15.21490288,  30.84970284],\n",
       "        [ 22.74432755,   8.09888458,  34.54758835, ...,  20.02509308,\n",
       "          17.19027138,  32.89435196],\n",
       "        [ 21.35910606,  10.4706583 ,  35.51891327, ...,  18.40789795,\n",
       "          19.30864143,  24.53445816],\n",
       "        ..., \n",
       "        [ 20.67406082,   7.23206043,  34.647686  , ...,  19.00139618,\n",
       "          19.30984688,  32.07115173],\n",
       "        [ 19.72752571,   8.84017944,  30.79948997, ...,  21.95375061,\n",
       "          15.18465996,  29.26916313],\n",
       "        [ 18.98001289,   6.6545229 ,  35.66272736, ...,  23.46886063,\n",
       "          19.93996811,  33.1199379 ]], dtype=float32),\n",
       " array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sess.run(C)\n",
    "i=2\n",
    "sess.run([cost,dist,choice], feed_dict={X: teX[i:i+128], Y: teY[:nBatch], alpha:alpha_})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.84242296,  0.        ,\n",
       "        0.        ,  0.44587955,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.873519  ,  0.        ,  0.        ,  0.        ,\n",
       "        0.21491808,  0.0541196 ,  0.        ,  0.        ,  0.87756169,\n",
       "        0.81012118,  0.        ,  0.41548687,  0.        ,  0.        ,\n",
       "        0.93266147,  1.1026715 ,  0.218925  ,  0.88959116,  0.        ,\n",
       "        0.32422784,  0.        ,  0.        ,  0.64849174,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.46091956,\n",
       "        0.06784925,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        1.62431395,  0.        ,  1.72730875,  0.        ,  0.77251101,\n",
       "        1.16562068,  0.49161229,  0.42182651,  0.        ,  0.        ,\n",
       "        0.61478388,  1.15518892,  0.        ,  0.        ,  0.        ,\n",
       "        0.60864657,  0.        ,  0.557289  ,  0.        ,  0.        ,\n",
       "        0.03772029,  0.1366367 ,  0.        ,  0.        ,  0.        ,\n",
       "        0.33254308,  0.        ,  0.86542612,  1.5700804 ,  0.        ,\n",
       "        0.36782494,  0.4792279 ,  0.1719597 ,  0.63508803,  0.        ,\n",
       "        0.        ,  1.57604957,  0.6342237 ,  1.08542871,  0.52667755,\n",
       "        1.70568776,  0.09515563,  0.        ,  0.        ,  1.24770081,\n",
       "        0.        ,  0.        ,  0.12227268,  0.        ,  0.        ,\n",
       "        0.04489902,  0.56755483,  1.33021438,  0.        ,  0.        ,\n",
       "        0.36260545,  0.95294893,  0.        ,  0.62553245,  0.50741702,\n",
       "        0.99629533,  0.        ,  0.        ,  0.23411161,  0.        ,\n",
       "        0.        ,  0.        ,  0.54071712,  0.        ,  0.1641017 ,\n",
       "        0.        ,  0.        ,  1.71970499,  0.93782884,  1.65679991,\n",
       "        0.        ,  1.01223159,  0.03476605,  0.        ,  0.        ,\n",
       "        0.21071663,  0.        ,  0.11713956,  0.02974216,  0.62420809,\n",
       "        1.9485352 ,  0.        ,  0.        ,  0.90911168,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.22722706,\n",
       "        0.        ,  0.        ,  0.52979219,  0.64941031,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.82281041,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        1.37666893,  0.        ,  0.        ,  0.        ,  0.7059294 ,\n",
       "        0.        ,  0.14485729,  0.08239395,  0.41250134,  0.        ,\n",
       "        1.44259202,  0.        ,  0.        ,  1.02258182,  0.87612265,\n",
       "        0.        ,  1.23438835,  0.        ,  0.        ,  0.46636471,\n",
       "        0.96740758,  0.        ,  0.        ,  0.44660959,  0.        ,\n",
       "        0.33688015,  0.        ,  0.        ,  0.        ,  1.01691496,\n",
       "        0.        ,  0.94324219,  0.86646837,  0.7984426 ,  1.41539049,\n",
       "        0.        ,  0.        ,  0.0099824 ,  0.26064715,  0.        ,\n",
       "        0.24057782,  1.01823413,  0.        ,  0.8256886 ,  0.        ,\n",
       "        0.        ,  0.18866047,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.95029187,  0.        ,  0.08798336,  0.07974675,\n",
       "        0.        ,  0.        ,  0.        ,  0.57558644,  0.        ,\n",
       "        1.52802861,  1.38904679,  0.        ,  0.        ,  0.        ,\n",
       "        0.71415031,  0.        ,  1.27121723,  0.        ,  0.48884326,\n",
       "        0.99167126,  0.97952968,  0.77189237,  0.20106958,  0.41399062,\n",
       "        0.        ,  1.26463342,  0.38815221,  0.        ,  0.        ,\n",
       "        0.17022943,  1.4747858 ,  0.08467376,  1.74882722,  0.93065691,\n",
       "        0.        ,  1.4791491 ,  0.        ,  0.11264039,  0.        ,\n",
       "        1.35320675,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.93124425,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.13932237,  0.97909093,\n",
       "        1.30078077,  0.        ,  0.        ,  0.46473786,  1.65802217,\n",
       "        0.53021371,  1.55706394,  0.        ,  0.        ,  0.        ,\n",
       "        0.06230403,  0.        ,  0.88070583,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  1.56290841,  0.        ,\n",
       "        0.10822492,  0.        ,  0.16125122,  0.7546168 ,  0.50364733,\n",
       "        0.89097172,  0.        ,  0.        ,  0.06887343,  1.28510535,\n",
       "        1.12801933,  1.48788071,  0.36456102,  0.        ,  0.85937709,\n",
       "        0.95446151,  0.        ,  0.15255126,  0.81309021,  1.74385631,\n",
       "        0.84379846,  0.        ,  0.22943017,  0.41364506,  0.80592704,\n",
       "        1.61112821,  0.        ,  0.58104759,  0.        ,  0.        ,\n",
       "        0.        ,  0.20575638,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  1.02702725,  0.        ,  0.68264276,  0.        ,\n",
       "        0.        ,  0.        ,  1.58879077,  0.25461385,  0.12387926,\n",
       "        0.10987967,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.01959141,  0.18626475,  0.        ,  2.0223341 ,  0.        ,\n",
       "        0.64330882,  0.57659519,  0.        ,  0.98988569,  0.        ,\n",
       "        0.        ,  0.81250983,  0.        ,  0.57317615,  0.04994679,\n",
       "        0.09019689,  0.25731465,  0.        ,  0.        ,  0.        ,\n",
       "        0.58674741,  0.64478254,  0.        ,  0.        ,  0.85710561,\n",
       "        0.        ,  0.        ,  0.15868872,  0.        ,  0.40424144,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.17042255,\n",
       "        1.12915695,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.81552148,  1.64177024,  1.40935576,  0.14639598,  0.66276336,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  1.59801114,  0.        ,  0.        ,  0.        ,\n",
       "        0.80160201,  1.15092981,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.30857241,  0.        ,  0.        ,  0.97347617,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.39136988,  0.        ,  0.        ,  1.55136621,  0.24089558,\n",
       "        0.05499316,  0.15334082,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  2.00003958,  0.        ,\n",
       "        0.        ,  0.        ,  0.39572293,  0.14518896,  0.78813934,\n",
       "        0.        ,  0.61756253,  1.60607135,  0.        ,  0.        ,\n",
       "        0.21395566,  0.26100633,  0.        ,  1.35201728,  0.        ,\n",
       "        0.        ,  2.03600478,  0.25966012,  0.        ,  0.        ,\n",
       "        0.        ,  0.02620857,  0.        ,  0.23344809,  0.        ,\n",
       "        0.99653846,  1.98420918,  0.75246561,  0.70824766,  0.95606881,\n",
       "        0.        ,  0.0866324 ,  0.        ,  0.        ,  0.70717806,\n",
       "        0.80215263,  0.        ,  0.        ,  0.        ,  0.14461479,\n",
       "        0.48748937,  0.        ,  0.        ,  0.85590374,  0.53843701,\n",
       "        0.7682094 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.64338201,  0.        ,  0.        ,  2.27945352,  0.36373144,\n",
       "        0.        ,  0.25234538,  0.        ,  0.        ,  0.22277021,\n",
       "        0.0355176 ,  0.        ,  0.        ,  0.23681948,  0.19244525,\n",
       "        1.29929447,  0.        ,  0.70705503,  0.43717366,  0.        ,\n",
       "        0.        ,  0.        ,  1.17721641,  0.        ,  0.        ,\n",
       "        2.44198108,  0.        ,  0.526559  ,  1.0877459 ,  0.40386826,\n",
       "        1.60464191,  0.        ,  0.82852882,  1.00405216,  1.25122046,\n",
       "        0.        ,  0.        ,  0.        ,  0.41807795,  0.        ,\n",
       "        1.09609556,  1.32068765,  0.08965058,  0.        ,  0.        ,\n",
       "        1.14817095,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([FC,,feed_dict={X: teX[i:i+128], Y: teY[:nBatch], alpha:alpha_})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t=C_acc\n",
    "t=t.reshape(100,28,28,10) \n",
    "\n",
    "t2 = np.zeros((2800,280))\n",
    "for i in range(100):\n",
    "    for j in range(10):\n",
    "        t2[i*28:(i+1)*28,j*28:(j+1)*28] = t[i,:,:,j]\n",
    "plt.figure()\n",
    "plt.imshow(t2[:,:],cmap = plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "Kcents = 10\n",
    "km = KMeans(init='random',n_clusters=Kcents, n_init=5, n_jobs=8)\n",
    "#km = KMeans(init='k-means++',n_clusters=Kcents, n_init=5, n_jobs=8)\n",
    "km.fit(trX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "km.cluster_centers_.shape\n",
    "plotcent(km.cluster_centers_.reshape((10,28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.randint(0,len(trX),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
